{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2023-07-20 13:37:57.041363\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import datetime as datetime\n",
                "import pprint\n",
                "import os\n",
                "import psycopg2\n",
                "import requests\n",
                "import boto3\n",
                "import dotenv\n",
                "from requests import get \n",
                "from bs4 import BeautifulSoup\n",
                "from urllib.parse import urlparse, parse_qs, urlunparse\n",
                "from bson.json_util import dumps\n",
                "from IPython.display import display, Image\n",
                "from IPython.display import HTML\n",
                "\n",
                "# An example of getting current date\n",
                "\n",
                "currDate = datetime.datetime.now()\n",
                " \n",
                "print(currDate)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the environment variables from the .env file\n",
                "dotenv.load_dotenv()\n",
                "\n",
                "scrape_url = os.getenv('SCRAPE_URL')\n",
                "# print(scrape_url)\n",
                "\n",
                "POSTGRES_URL = os.environ.get('POSTGRES_URL')\n",
                "# print(POSTGRES_URL)\n",
                "\n",
                "FINAL_POSTGRES_URL = os.environ.get('FINAL_POSTGRES_URL')\n",
                "# print(POSTGRES_URL)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# response = requests.get(scrape_url)\n",
                "\n",
                "# # Create BeautifulSoup object\n",
                "# soup = BeautifulSoup(response.text, \"html.parser\")\n",
                "\n",
                "# body_tag = soup.body\n",
                "\n",
                "# print(body_tag)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# example_img = \"https://pbs.twimg.com/media/FyV7PAEWwAADDKD.jpg\"\n",
                "# # display(Image(example_img))\n",
                "# display(Image(example_img, width=200))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Find all the lines (within <a> tags) in the webpage\n",
                "# lines = soup.find_all(\"a\")\n",
                "\n",
                "# # Iterate over the lines and print line number, content, and type (link or image)\n",
                "# for i, line in enumerate(lines, start=1):\n",
                "#     line_content = line.get_text()\n",
                "#     line_type = \"Link\"\n",
                "#     line_url = line.get(\"href\")\n",
                "\n",
                "#     if line.find(\"img\"):\n",
                "#         line_type = \"Image\"\n",
                "\n",
                "#     print(f\"Line {i}: {line_content} [{line_type}]\")\n",
                "#     print(f\"URL: {line_url}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# soup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip show selenium"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "__init__() got an unexpected keyword argument 'executable_path'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m chrome_options\u001b[39m.\u001b[39mbinary_location \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/snap/bin/chromium\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Optional if Chromium is not in the system's PATH\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# driver = webdriver.Chrome(executable_path=driver_path, chrome_options=chrome_options)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# driver = webdriver.Chrome(executable_path=driver_path, options=chrome_options)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mChrome(executable_path\u001b[39m=\u001b[39;49mdriver_path, options\u001b[39m=\u001b[39;49mchrome_options)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Step 2: Use Selenium to load the page and execute JavaScript\u001b[39;00m\n\u001b[1;32m     17\u001b[0m driver\u001b[39m.\u001b[39mget(scrape_url)\n",
                        "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'executable_path'"
                    ]
                }
            ],
            "source": [
                "from selenium import webdriver\n",
                "from selenium.webdriver.chrome.options import Options\n",
                "from bs4 import BeautifulSoup\n",
                "\n",
                "# Specify the path to the uploaded chromedriver executable\n",
                "driver_path = \"/usr/local/bin/chromium\"\n",
                "\n",
                "# Set up the WebDriver (Chromium in this example)\n",
                "chrome_options = Options()\n",
                "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no visible browser)\n",
                "chrome_options.binary_location = \"/snap/bin/chromium\"  # Optional if Chromium is not in the system's PATH\n",
                "# driver = webdriver.Chrome(executable_path=driver_path, chrome_options=chrome_options)\n",
                "# driver = webdriver.Chrome(executable_path=driver_path, options=chrome_options)\n",
                "driver = webdriver.Chrome(executable_path=driver_path, options=chrome_options)\n",
                "\n",
                "# Step 2: Use Selenium to load the page and execute JavaScript\n",
                "driver.get(scrape_url)\n",
                "\n",
                "# Optionally, you can add some waits to ensure the JavaScript content is loaded completely\n",
                "# For example, you can use WebDriverWait:\n",
                "# from selenium.webdriver.support.ui import WebDriverWait\n",
                "# from selenium.webdriver.common.by import By\n",
                "# from selenium.webdriver.support import expected_conditions as EC\n",
                "# wait = WebDriverWait(driver, 10)\n",
                "# wait.until(EC.presence_of_element_located((By.XPATH, \"xpath-of-some-element-on-the-page\")))\n",
                "\n",
                "# Step 3: Pass the page source to Beautiful Soup\n",
                "page_source = driver.page_source\n",
                "soup = BeautifulSoup(page_source, \"html.parser\")\n",
                "\n",
                "# Step 4: Use Beautiful Soup to extract data from the page\n",
                "# Perform your scraping tasks here using Beautiful Soup as usual\n",
                "# For example:\n",
                "# title = soup.title.text\n",
                "# links = soup.find_all(\"a\")\n",
                "\n",
                "# Step 5: Close the WebDriver\n",
                "driver.quit()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
